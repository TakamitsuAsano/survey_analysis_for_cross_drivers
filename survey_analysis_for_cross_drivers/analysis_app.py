import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os
import io
import graphviz
import json
import re
from streamlit_gsheets import GSheetsConnection
from st_copy_to_clipboard import st_copy_to_clipboard

# --- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
def setup_japanese_font():
    font_path = "/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf"
    if os.path.exists(font_path):
        try:
            fm.fontManager.addfont(font_path)
            plt.rcParams['font.family'] = 'IPAexGothic'
        except Exception as e:
            st.error(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    else:
        try:
            plt.rcParams['font.family'] = 'Hiragino Sans'
        except:
            pass

setup_japanese_font()

# --- æ±ºå®šæœ¨ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›ã™ã‚‹é–¢æ•° ---
def get_decision_tree_data(clf, feature_names, class_names):
    n_nodes = clf.tree_.node_count
    feature = clf.tree_.feature
    threshold = clf.tree_.threshold
    impurity = clf.tree_.impurity
    n_node_samples = clf.tree_.n_node_samples
    value = clf.tree_.value
    children_left = clf.tree_.children_left
    children_right = clf.tree_.children_right

    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)
    is_leaves = np.zeros(shape=n_nodes, dtype=bool)
    stack = [(0, 0)]
    
    while len(stack) > 0:
        node_id, depth = stack.pop()
        node_depth[node_id] = depth
        is_split_node = children_left[node_id] != children_right[node_id]
        if is_split_node:
            stack.append((children_left[node_id], depth + 1))
            stack.append((children_right[node_id], depth + 1))
        else:
            is_leaves[node_id] = True

    data = []
    for i in range(n_nodes):
        node_type = "Leaf" if is_leaves[i] else "Node"
        if i == 0: node_type = "Root"
        
        if feature[i] != -2:
            feat_name = feature_names[feature[i]]
            th = threshold[i]
            condition = f"{feat_name} <= {th:.3f}"
        else:
            feat_name = None
            th = None
            condition = None
        
        val = value[i][0]
        class_idx = np.argmax(val)
        pred_class = class_names[class_idx] if class_names is not None else str(class_idx)
        
        row = {
            "Node_ID": i,
            "Depth": node_depth[i],
            "Type": node_type,
            "Condition": condition,
            "Feature": feat_name,
            "Threshold": th,
            "Gini": f"{impurity[i]:.4f}",
            "Samples": n_node_samples[i],
            "Value": str(list(map(int, val))),
            "Predicted_Class": pred_class
        }
        data.append(row)
    
    return pd.DataFrame(data)

# --- ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ç¾¤ (Tab7ç”¨) ---
def cb_update_cross(idx, col):
    st.session_state["sb_cross_index"] = idx
    st.session_state["sb_cross_col"] = col

def cb_update_reg(tgt, feats):
    st.session_state["sb_reg_target"] = tgt
    st.session_state["sb_reg_feature"] = feats
    st.session_state['ai_msg_reg'] = True

def cb_update_tree(tgt, feats):
    st.session_state["sb_tree_target"] = tgt
    st.session_state["sb_tree_feature"] = feats
    st.session_state['ai_msg_tree'] = True

def cb_update_cluster(feats):
    st.session_state["sb_cluster_features"] = feats
    st.session_state['ai_msg_cluster'] = True

# --- é¸æŠè‚¢ã®ä¸¦ã³é †ã‚’å¼·åˆ¶ã™ã‚‹é–¢æ•° ---
def enforce_likert_order(df, col_name):
    likert_order_master = [
        "éå¸¸ã«ãã†æ€ã†", "å¼·ããã†æ€ã†", "ã¨ã¦ã‚‚ãã†æ€ã†", "ãã†æ€ã†", "ã‹ãªã‚Šãã†æ€ã†",
        "ã‚„ã‚„ãã†æ€ã†", "ã©ã¡ã‚‰ã‹ã¨ã„ãˆã°ãã†æ€ã†", "ã©ã¡ã‚‰ã‹ã¨è¨€ãˆã°ãã†æ€ã†",
        "ã©ã¡ã‚‰ã¨ã‚‚ã„ãˆãªã„", "ã©ã¡ã‚‰ã¨ã‚‚è¨€ãˆãªã„", "æ™®é€š",
        "ã©ã¡ã‚‰ã‹ã¨ã„ãˆã°ãã†æ€ã‚ãªã„", "ã©ã¡ã‚‰ã‹ã¨è¨€ãˆã°ãã†æ€ã‚ãªã„", "ã‚ã¾ã‚Šãã†æ€ã‚ãªã„",
        "ãã†æ€ã‚ãªã„", "å…¨ããã†æ€ã‚ãªã„", "å…¨ãæ€ã‚ãªã„"
    ]
    unique_vals = df[col_name].dropna().unique()
    sorter = [x for x in likert_order_master if x in unique_vals]
    remaining = [x for x in unique_vals if x not in sorter]
    sorted_remaining = sorted(remaining, key=lambda x: str(x))
    final_order = sorter + sorted_remaining
    
    if len(sorter) >= 2:
        return pd.Categorical(df[col_name], categories=final_order, ordered=True)
    else:
        return df[col_name]

# ---------------------------------------

st.set_page_config(page_title="ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆåˆ†æ & ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³", layout="wide")
st.title("ğŸ“Š ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆåˆ†æ & ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
st.sidebar.header("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
input_method = st.sidebar.radio("ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’é¸æŠ", ["ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"])

df = None

if input_method == "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
    uploaded_file = st.sidebar.file_uploader("Excelã¾ãŸã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.sidebar.success("ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸï¼")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

elif input_method == "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ":
    st.sidebar.info("äº‹å‰ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã€Œå…±æœ‰ã€ã«ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    sheet_url = st.sidebar.text_input("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URLã‚’å…¥åŠ›")
    
    if sheet_url:
        try:
            conn = st.connection("gsheets", type=GSheetsConnection)
            df = conn.read(spreadsheet=sheet_url, ttl=0)
            st.sidebar.success("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šæˆåŠŸï¼")
        except Exception as e:
            st.sidebar.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: Secretsã®è¨­å®šã¾ãŸã¯URLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n{e}")

# --- åˆ†æãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if df is not None:
    df = df.dropna(how='all')

    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ç¢ºèª", 
        "ğŸ“ˆ ã‚¯ãƒ­ã‚¹é›†è¨ˆ", 
        "ğŸš€ è¦å› (ãƒ‰ãƒ©ã‚¤ãƒãƒ¼)åˆ†æ", 
        "ğŸŒ³ æ±ºå®šæœ¨åˆ†æ", 
        "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ",
        "ğŸ“– åˆ†ææ‰‹æ³•ã®è§£èª¬(ç”¨èªé›†)",
        "ğŸ¤– AIåˆ†æã‚¢ã‚·ã‚¹ãƒˆ"
    ])

    # --- ã‚¿ãƒ–1: ãƒ‡ãƒ¼ã‚¿ç¢ºèª ---
    with tab1:
        st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(df)
        st.info(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")

    # --- ã‚¿ãƒ–2: ã‚¯ãƒ­ã‚¹é›†è¨ˆ ---
    with tab2:
        st.subheader("ã‚¯ãƒ­ã‚¹é›†è¨ˆã¨å¯è¦–åŒ–")
        
        col1, col2 = st.columns(2)
        with col1:
            index_col = st.selectbox("è¡Œï¼ˆIndexï¼‰ã‚’é¸æŠ", df.columns, index=0, key="sb_cross_index")
        with col2:
            columns_col = st.selectbox("åˆ—ï¼ˆColumnï¼‰ã‚’é¸æŠ", df.columns, index=min(1, len(df.columns)-1), key="sb_cross_col")

        if index_col == columns_col:
            st.warning("âš ï¸ è¡Œã¨åˆ—ã«ã¯ç•°ãªã‚‹é …ç›®ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            df_cross = df.copy()
            df_cross[index_col] = enforce_likert_order(df_cross, index_col)
            df_cross[columns_col] = enforce_likert_order(df_cross, columns_col)
            
            calc_type = st.radio("é›†è¨ˆå€¤ã®è¡¨ç¤ºå½¢å¼", ["åº¦æ•°ï¼ˆäººæ•°ï¼‰", "è¡Œãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼ˆæ¨ªè¨ˆ=100%ï¼‰", "åˆ—ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼ˆç¸¦è¨ˆ=100%ï¼‰"], horizontal=True)
            
            if calc_type == "åº¦æ•°ï¼ˆäººæ•°ï¼‰":
                cross_tab = pd.crosstab(df_cross[index_col], df_cross[columns_col])
                fmt = ""
                val_name = "äººæ•°"
            elif calc_type == "è¡Œãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼ˆæ¨ªè¨ˆ=100%ï¼‰":
                cross_tab = pd.crosstab(df_cross[index_col], df_cross[columns_col], normalize='index') * 100
                cross_tab = cross_tab.round(1)
                fmt = ".1f"
                val_name = "å‰²åˆ(%)"
            else:
                cross_tab = pd.crosstab(df_cross[index_col], df_cross[columns_col], normalize='columns') * 100
                cross_tab = cross_tab.round(1)
                fmt = ".1f"
                val_name = "å‰²åˆ(%)"

            st.write("##### é›†è¨ˆè¡¨")
            
            copy_text = cross_tab.to_csv(sep='\t')
            st_copy_to_clipboard(copy_text, "ğŸ“‹ è¡¨ã‚’ã‚³ãƒ”ãƒ¼ (ãƒ˜ãƒƒãƒ€ãƒ¼ä»˜)", "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸï¼")
            
            st.dataframe(cross_tab) 

            graph_type = st.radio("ã‚°ãƒ©ãƒ•ã®ç¨®é¡", ["ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•"], horizontal=True)
            if graph_type == "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—":
                fig = px.imshow(cross_tab, text_auto=fmt if fmt else True, aspect="auto", color_continuous_scale='Blues')
            else:
                cross_tab_reset = cross_tab.reset_index().melt(id_vars=index_col, var_name=columns_col, value_name=val_name)
                fig = px.bar(cross_tab_reset, x=index_col, y=val_name, color=columns_col, title=f"{index_col} Ã— {columns_col}", text_auto=fmt if fmt else True)
            st.plotly_chart(fig, use_container_width=True)

    # --- ã‚¿ãƒ–3: è¦å› (ãƒ‰ãƒ©ã‚¤ãƒãƒ¼)åˆ†æ ---
    with tab3:
        st.subheader("ğŸš€ è¦å› ï¼ˆãƒ‰ãƒ©ã‚¤ãƒãƒ¼ï¼‰åˆ†æï¼šã‚ªãƒƒã‚ºæ¯”")
        
        st.info("""
        **ğŸ’¡ æ•°å€¤ã®è¦‹æ–¹ï¼ˆã‚ªãƒƒã‚ºæ¯”ï¼‰**
        * **1.0 ã‚ˆã‚Šå¤§ãã„**: ãã®è¦å› ãŒçµæœã‚’**ä¿ƒé€²**ã—ã¾ã™ã€‚ï¼ˆä¾‹ï¼š2.0ãªã‚‰ã€ãã®è¦å› ãŒã‚ã‚‹ã¨çµæœãŒ2å€èµ·ã“ã‚Šã‚„ã™ã„ï¼‰
        * **1.0 ã‚ˆã‚Šå°ã•ã„**: ãã®è¦å› ãŒçµæœã‚’**æŠ‘åˆ¶**ã—ã¾ã™ã€‚ï¼ˆä¾‹ï¼š0.5ãªã‚‰ã€ãã®è¦å› ãŒã‚ã‚‹ã¨çµæœãŒåŠåˆ†ã—ã‹èµ·ã“ã‚‰ãªã„ï¼‰
        """)

        col1, col2 = st.columns(2)
        with col1:
            target_col_reg = st.selectbox("ç›®çš„å¤‰æ•°ï¼ˆåˆ†æã—ãŸã„çµæœï¼‰", df.columns, index=0, key="sb_reg_target")
        with col2:
            valid_options_reg = [c for c in df.columns if c != target_col_reg]
            
            if "sb_reg_feature" in st.session_state:
                current_selection = st.session_state["sb_reg_feature"]
                safe_selection = [x for x in current_selection if x in valid_options_reg]
                st.session_state["sb_reg_feature"] = safe_selection

            default_feats_reg = [c for c in df.columns if c != df.columns[0]][:5]
            safe_default_reg = [x for x in default_feats_reg if x in valid_options_reg]

            feature_cols_reg = st.multiselect(
                "èª¬æ˜å¤‰æ•°ï¼ˆèƒŒæ™¯ãƒ»è¦å› ã¨æ€ã‚ã‚Œã‚‹é …ç›®ï¼‰", 
                valid_options_reg, 
                default=safe_default_reg, 
                key="sb_reg_feature"
            )

        if 'ai_msg_reg' in st.session_state and st.session_state['ai_msg_reg']:
            st.info("âœ… AIãŒãŠã™ã™ã‚è¨­å®šã‚’åæ˜ ã—ã¾ã—ãŸã€‚ä¸‹ã®ã€Œè¦å› åˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            st.session_state['ai_msg_reg'] = False

        if st.button("è¦å› åˆ†æã‚’å®Ÿè¡Œ"):
            if not feature_cols_reg:
                st.warning("èª¬æ˜å¤‰æ•°ã‚’1ã¤ä»¥ä¸Šé¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                try:
                    df_reg = df[[target_col_reg] + feature_cols_reg].dropna()
                    
                    for col in df_reg.columns:
                        if df_reg[col].dtype == 'object':
                            le = LabelEncoder()
                            df_reg[col] = df_reg[col].astype(str)
                            df_reg[col] = le.fit_transform(df_reg[col])

                    X = df_reg[feature_cols_reg]
                    y = df_reg[target_col_reg]
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)

                    model = LogisticRegression(max_iter=1000)
                    model.fit(X_scaled, y)

                    if model.coef_.shape[0] > 1: coefs = model.coef_[-1]
                    else: coefs = model.coef_[0]
                    
                    odds_ratios = np.exp(coefs)
                    res_df = pd.DataFrame({"è¦å› ": feature_cols_reg, "ã‚ªãƒƒã‚ºæ¯”": odds_ratios}).sort_values(by="ã‚ªãƒƒã‚ºæ¯”", ascending=True)

                    st.write(f"### ã€Œ{target_col_reg}ã€ã¸ã®å½±éŸ¿åº¦ï¼ˆã‚ªãƒƒã‚ºæ¯”ï¼‰")
                    fig = px.bar(res_df, x="ã‚ªãƒƒã‚ºæ¯”", y="è¦å› ", orientation='h', 
                                 title=f"ã€Œ{target_col_reg}ã€ã«å¯¾ã™ã‚‹ã‚ªãƒƒã‚ºæ¯”ï¼ˆ1.0ãŒåŸºæº–ï¼‰",
                                 color="ã‚ªãƒƒã‚ºæ¯”", color_continuous_scale="RdBu_r", color_continuous_midpoint=1.0)
                    fig.add_vline(x=1.0, line_width=2, line_dash="dash", line_color="black")
                    st.plotly_chart(fig, use_container_width=True)
                    
                    st.write("##### è©³ç´°ãƒ‡ãƒ¼ã‚¿")
                    st_copy_to_clipboard(res_df.to_csv(sep='\t'), "ğŸ“‹ æ•°å€¤ã‚’ã‚³ãƒ”ãƒ¼", "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                    st.dataframe(res_df)

                except Exception as e:
                    st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    # --- ã‚¿ãƒ–4: æ±ºå®šæœ¨åˆ†æ ---
    with tab4:
        st.subheader("æ±ºå®šæœ¨åˆ†æ")
        st.caption("ğŸ’¡ å›³ã¯ãƒã‚¦ã‚¹ãƒ›ã‚¤ãƒ¼ãƒ«ã§**æ‹¡å¤§ãƒ»ç¸®å°**ã€ãƒ‰ãƒ©ãƒƒã‚°ã§**ç§»å‹•**ãŒã§ãã¾ã™ã€‚")
        
        col1, col2 = st.columns(2)
        with col1:
            target_col_tree = st.selectbox("ç›®çš„å¤‰æ•°ï¼ˆçµæœï¼‰", df.columns, index=0, key="sb_tree_target")
        with col2:
            valid_options = [c for c in df.columns if c != target_col_tree]
            
            if "sb_tree_feature" in st.session_state:
                current_selection = st.session_state["sb_tree_feature"]
                safe_selection = [x for x in current_selection if x in valid_options]
                st.session_state["sb_tree_feature"] = safe_selection

            default_feats = [c for c in df.columns if c != df.columns[0]][:3]
            safe_default = [x for x in default_feats if x in valid_options]

            feature_cols_tree = st.multiselect(
                "èª¬æ˜å¤‰æ•°ï¼ˆè¦å› ï¼‰", 
                valid_options, 
                default=safe_default, 
                key="sb_tree_feature"
            )

        if 'ai_msg_tree' in st.session_state and st.session_state['ai_msg_tree']:
            st.info("âœ… AIãŒãŠã™ã™ã‚è¨­å®šã‚’åæ˜ ã—ã¾ã—ãŸã€‚ä¸‹ã®ã€Œæ±ºå®šæœ¨åˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            st.session_state['ai_msg_tree'] = False

        if st.button("æ±ºå®šæœ¨åˆ†æã‚’å®Ÿè¡Œ"):
            if not feature_cols_tree:
                st.warning("èª¬æ˜å¤‰æ•°ã‚’1ã¤ä»¥ä¸Šé¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                try:
                    df_ml = df.copy()
                    
                    class_names_list = None
                    if df_ml[target_col_tree].dtype == 'object':
                        le_target = LabelEncoder()
                        df_ml[target_col_tree] = df_ml[target_col_tree].astype(str)
                        df_ml[target_col_tree] = le_target.fit_transform(df_ml[target_col_tree])
                        class_names_list = le_target.classes_.astype(str).tolist()
                    else:
                        class_names_list = sorted(df_ml[target_col_tree].unique().astype(str).tolist())

                    for col in feature_cols_tree:
                        if df_ml[col].dtype == 'object':
                            df_ml[col] = df_ml[col].astype(str)
                            le = LabelEncoder()
                            df_ml[col] = le.fit_transform(df_ml[col])

                    df_ml = df_ml.dropna(subset=[target_col_tree] + feature_cols_tree)
                    X = df_ml[feature_cols_tree]
                    y = df_ml[target_col_tree]

                    clf = DecisionTreeClassifier(max_depth=3, random_state=42)
                    clf.fit(X, y)

                    tree_rules = export_text(clf, feature_names=feature_cols_tree)
                    st.write("##### ğŸ“‹ åˆ†å²æ¡ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆè©³ç´°")
                    st_copy_to_clipboard(tree_rules, "ğŸ“‹ åˆ†å²ãƒ«ãƒ¼ãƒ«ã‚’ã‚³ãƒ”ãƒ¼", "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                    st.code(tree_rules)

                    dot_data = export_graphviz(
                        clf, out_file=None, feature_names=feature_cols_tree, class_names=class_names_list,
                        filled=True, rounded=True, special_characters=True, fontname="IPAexGothic"
                    )
                    st.graphviz_chart(dot_data)
                    
                    try:
                        graph = graphviz.Source(dot_data)
                        png_bytes = graph.pipe(format='png')
                        st.download_button("ğŸ“¥ æ±ºå®šæœ¨ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PNG)", png_bytes, "decision_tree.png", "image/png")
                    except: pass

                    st.divider()
                    st.write("##### ğŸ“Š æ±ºå®šæœ¨ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                    tree_df = get_decision_tree_data(clf, feature_cols_tree, class_names_list)
                    st.download_button("ğŸ“¥ æ±ºå®šæœ¨ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", tree_df.to_csv(index=False).encode('utf-8_sig'), "decision_tree_data.csv", "text/csv")

                except Exception as e:
                    st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    # --- ã‚¿ãƒ–5: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ ---
    with tab5:
        st.subheader("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        
        cluster_features = st.multiselect(
            "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã«ä½¿ã†å¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„",
            df.columns,
            default=df.columns[:5].tolist(),
            key="sb_cluster_features"
        )
        
        n_clusters = st.slider("åˆ†é¡ã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—æ•°", 2, 10, 4)

        if 'ai_msg_cluster' in st.session_state and st.session_state['ai_msg_cluster']:
            st.info("âœ… AIãŒãŠã™ã™ã‚è¨­å®šã‚’åæ˜ ã—ã¾ã—ãŸã€‚ä¸‹ã®ã€Œã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            st.session_state['ai_msg_cluster'] = False

        if st.button("ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‚’å®Ÿè¡Œ"):
            if not cluster_features:
                st.warning("å¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„")
            else:
                try:
                    df_cluster = df[cluster_features].dropna()
                    
                    for col in df_cluster.columns:
                        if df_cluster[col].dtype == 'object':
                            le = LabelEncoder()
                            df_cluster[col] = df_cluster[col].astype(str)
                            df_cluster[col] = le.fit_transform(df_cluster[col])

                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(df_cluster)
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                    clusters = kmeans.fit_predict(X_scaled)
                    
                    df['Cluster'] = clusters
                    df['Cluster_Name'] = df['Cluster'].apply(lambda x: f"ã‚°ãƒ«ãƒ¼ãƒ— {x+1}")
                    st.success(f"{n_clusters}ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡ã—ã¾ã—ãŸï¼")

                    cluster_means_numeric = df_cluster.copy()
                    cluster_means_numeric['Cluster_Name'] = df['Cluster_Name']
                    cluster_means_numeric = cluster_means_numeric.groupby('Cluster_Name').mean()

                    st.write("##### ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®å›ç­”å‚¾å‘ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰")
                    fig = px.imshow(cluster_means_numeric, text_auto=".2f", aspect="auto", color_continuous_scale="Viridis", title="ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã®ç‰¹å¾´æ¯”è¼ƒ")
                    st.plotly_chart(fig, use_container_width=True)

                    st.write("##### åˆ†é¡çµæœä»˜ããƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                    st.download_button("ğŸ“¥ åˆ†é¡çµæœä»˜ãCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", df.to_csv(index=False).encode('utf-8_sig'), 'clustered_data.csv', 'text/csv')
                    st.write("##### ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®äººæ•°")
                    st.dataframe(df['Cluster_Name'].value_counts().reset_index().rename(columns={'index':'Group', 'Cluster_Name':'Count'}))

                except Exception as e:
                    st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    # --- ã‚¿ãƒ–6: è§£èª¬ ---
    with tab6:
        st.header("ğŸ“– çµ±è¨ˆåˆ†ææ‰‹æ³•ã®è§£èª¬ã‚¬ã‚¤ãƒ‰")
        st.markdown("""
        ã“ã®ã‚¢ãƒ—ãƒªã§ä½¿ç”¨ã—ã¦ã„ã‚‹åˆ†ææ‰‹æ³•ã«ã¤ã„ã¦ã€ã€Œå°‚é–€çš„ãªèª¬æ˜ï¼ˆTechnicalï¼‰ã€ã¨ã€Œã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜ï¼ˆPlainï¼‰ã€ã‚’ä½µè¨˜ã—ã¦ã„ã¾ã™ã€‚
        å ±å‘Šæ›¸ä½œæˆã‚„ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®éš›ã«ã”æ´»ç”¨ãã ã•ã„ã€‚
        """)
        
        st.divider()

        # 1. ã‚¯ãƒ­ã‚¹é›†è¨ˆ
        st.subheader("1. ã‚¯ãƒ­ã‚¹é›†è¨ˆ (Cross Tabulation)")
        with st.expander("è©³ç´°ã‚’è¦‹ã‚‹"):
            st.markdown("""
            #### ğŸ›  ä½¿ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»åŸç†
            * **æ‰‹æ³•**: åˆ†å‰²è¡¨ (Contingency Table) ã®ä½œæˆ
            * **çµ±è¨ˆçš„èƒŒæ™¯**: 2ã¤ã®å¤‰æ•°ï¼ˆè³ªå•é …ç›®ï¼‰ã®é–“ã«ã€Œé–¢é€£ãŒã‚ã‚‹ã‹ã€ã‚’è¦‹ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™ã€‚å³å¯†ã«ã¯ã€Œã‚«ã‚¤äºŒä¹—æ¤œå®š (Chi-square test)ã€ã‚’ç”¨ã„ã¦ã€ãã®åã‚ŠãŒå¶ç„¶ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚

            #### ğŸ’¡ ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜
            * **ã“ã‚Œä½•ï¼Ÿ**: ã€Œå¹´ä»£åˆ¥ Ã— æº€è¶³åº¦ã€ã®ã‚ˆã†ã«ã€2ã¤ã®è³ªå•ã‚’æ›ã‘åˆã‚ã›ã¦è¡¨ã«ã™ã‚‹æœ€ã‚‚åŸºæœ¬çš„ãªåˆ†æã§ã™ã€‚
            * **ç›®çš„**: å…¨ä½“ã ã‘ã§ã¯è¦‹ãˆãªã„ã€ç‰¹å®šã®å±æ€§ï¼ˆç”·å¥³ã€å¹´ä»£ãªã©ï¼‰ã”ã¨ã®é•ã„ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚
            * **ã€Œæœ‰æ„å·®ï¼ˆæ„å‘³ã®ã‚ã‚‹å·®ï¼‰ã€ã®ç›®å®‰**:
                * ä¸€èˆ¬çš„ã«ã€æ¯”è¼ƒã—ãŸã„ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§ **10%ä»¥ä¸Šã®å·®** ãŒã‚ã‚Œã°ã€ã€Œå·®ãŒã‚ã‚‹ã€ã¨è¦‹ãªã—ã¦è‰¯ã„ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã§ã™ï¼ˆãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ç¾å ´ãƒ¬ãƒ™ãƒ«ï¼‰ã€‚
            """)

        # 2. ãƒ‰ãƒ©ã‚¤ãƒãƒ¼åˆ†æ
        st.subheader("2. ãƒ‰ãƒ©ã‚¤ãƒãƒ¼åˆ†æ / è¦å› åˆ†æ")
        with st.expander("è©³ç´°ã‚’è¦‹ã‚‹"):
            st.markdown("""
            #### ğŸ›  ä½¿ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»åŸç†
            * **æ‰‹æ³•**: **ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æ (Logistic Regression)**
            * **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: scikit-learnã® `LogisticRegression` ã‚’ä½¿ç”¨ã€‚
            * **çµ±è¨ˆçš„èƒŒæ™¯**: çµæœãŒã€ŒYes/Noï¼ˆè²·ã£ãŸ/è²·ã‚ãªã„ï¼‰ã€ã®ã‚ˆã†ãª2å€¤ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€é€šå¸¸ã®å›å¸°åˆ†æã¯ä½¿ãˆã¾ã›ã‚“ã€‚ãã“ã§ã€Œç¢ºç‡ã€ã‚’äºˆæ¸¬ã™ã‚‹ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ç”¨ã„ã¾ã™ã€‚
            * **ä¿‚æ•°ã®å¤‰æ›**: ç®—å‡ºã•ã‚ŒãŸã€Œåå›å¸°ä¿‚æ•°ã€ã‚’ã€æŒ‡æ•°å¤‰æ›ï¼ˆ$e^x$ï¼‰ã™ã‚‹ã“ã¨ã§ **ã€Œã‚ªãƒƒã‚ºæ¯” (Odds Ratio)ã€** ã«å¤‰æ›ã—ã¦è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚

            #### ğŸ’¡ ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜
            * **ã“ã‚Œä½•ï¼Ÿ**: ã‚ã‚‹çµæœï¼ˆä¾‹ï¼šå•†å“ã‚’è²·ã£ãŸï¼‰ã«å¯¾ã—ã¦ã€ã©ã®è¦å› ãŒã©ã‚Œãã‚‰ã„å½±éŸ¿ã—ãŸã‹ã‚’æ•°å€¤åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚
            * **ç›®çš„**: æ–½ç­–ã®å„ªå…ˆé †ä½ã‚’æ±ºã‚ã‚‹ãŸã‚ã§ã™ã€‚ã€Œã“ã‚Œã‚’æ”¹å–„ã™ã‚Œã°ã€çµæœãŒã“ã‚Œã ã‘ä¼¸ã³ã‚‹ã€ã¨ã„ã†ãƒ¬ãƒãƒ¼ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚
            * **ã€Œã‚ªãƒƒã‚ºæ¯”ã€ã¨ã¯ï¼Ÿ**:
                * çµæœã®**ã€Œèµ·ã“ã‚Šã‚„ã™ã•ã€ãŒä½•å€ã«ãªã‚‹ã‹**ã‚’è¡¨ã™æ•°å€¤ã§ã™ã€‚
                * **1.0**: å½±éŸ¿ãªã—ï¼ˆãƒ—ãƒ©ãƒã‚¤ã‚¼ãƒ­ï¼‰ã€‚
                * **2.0**: ãã®è¦ç´ ãŒã‚ã‚‹ã¨ã€çµæœãŒ **2å€** èµ·ã“ã‚Šã‚„ã™ããªã‚‹ï¼ˆå¼·ã„ä¿ƒé€²è¦å› ï¼‰ã€‚
                * **0.5**: ãã®è¦ç´ ãŒã‚ã‚‹ã¨ã€çµæœãŒ **åŠåˆ†** ã—ã‹èµ·ããªããªã‚‹ï¼ˆå¼·ã„é˜»å®³è¦å› ï¼‰ã€‚
            """)

        # 3. æ±ºå®šæœ¨åˆ†æ
        st.subheader("3. æ±ºå®šæœ¨åˆ†æ (Decision Tree)")
        with st.expander("è©³ç´°ã‚’è¦‹ã‚‹"):
            st.markdown("""
            #### ğŸ›  ä½¿ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»åŸç†
            * **æ‰‹æ³•**: **CARTæ³• (Classification and Regression Trees)**
            * **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: scikit-learnã® `DecisionTreeClassifier` ã‚’ä½¿ç”¨ã€‚
            * **çµ±è¨ˆçš„èƒŒæ™¯**: ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹éš›ã€**ã€ŒGiniä¸ç´”åº¦ (Gini Impurity)ã€** ã¨ã„ã†æŒ‡æ¨™ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€Œã©ã‚Œã ã‘ç¶ºéº—ã«Yes/NoãŒåˆ†ã‹ã‚ŒãŸã‹ã€ã‚’è¨ˆç®—ã™ã‚‹ã‚‚ã®ã§ã€ã“ã®ä¸ç´”åº¦ãŒæœ€ã‚‚ä½ããªã‚‹æ¡ä»¶ã‚’æ¢ã—ã¦è‡ªå‹•çš„ã«åˆ†å²ã‚’ä½œã£ã¦ã„ã¾ã™ã€‚

            #### ğŸ’¡ ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜
            * **ã“ã‚Œä½•ï¼Ÿ**: ã€Œã‚‚ã—Aãªã‚‰Bã€ãã†ã§ãªã‘ã‚Œã°Cã€ã¨ã„ã†ã‚ˆã†ã«ã€çµæœã«è‡³ã‚‹æ¡ä»¶ã‚’ãƒ„ãƒªãƒ¼çŠ¶ã«åˆ†è§£ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚
            * **ç›®çš„**: è¤‡é›‘ãªè¦å› ã‚’æ•´ç†ã—ã€**ã€Œä¸€ç•ªå½±éŸ¿åŠ›ãŒå¤§ãã„æ¡ä»¶ã¯ä½•ã‹ï¼Ÿã€**ã‚’è¦–è¦šçš„ã«è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ä½¿ã„ã¾ã™ã€‚
            * **è¦‹æ–¹**:
                * **ä¸€ç•ªä¸Šã®åˆ†å²**: ã“ã‚ŒãŒ**æœ€ã‚‚é‡è¦ãªè¦å› **ã§ã™ã€‚ã“ã“ã‚’è¦‹ã‚‹ã ã‘ã§ã€çµæœã‚’å·¦å³ã™ã‚‹æœ€å¤§ã®ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‹ã‚Šã¾ã™ã€‚
            """)

        # 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        st.subheader("4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ (Cluster Analysis)")
        with st.expander("è©³ç´°ã‚’è¦‹ã‚‹"):
            st.markdown("""
            #### ğŸ›  ä½¿ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»åŸç†
            * **æ‰‹æ³•**: **K-Meansæ³• (K-å¹³å‡æ³• / ééšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ)**
            * **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: scikit-learnã® `KMeans` ã‚’ä½¿ç”¨ã€‚
            * **çµ±è¨ˆçš„èƒŒæ™¯**: ãƒ‡ãƒ¼ã‚¿ã‚’ $k$ å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹éš›ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸­å¿ƒï¼ˆé‡å¿ƒï¼‰ã‹ã‚‰ã®è·é›¢ãŒæœ€å°ã«ãªã‚‹ã‚ˆã†ã«è¨ˆç®—ã—ã¾ã™ã€‚æ•™å¸«ãªã—å­¦ç¿’ï¼ˆæ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒã„ã‚‰ãªã„åˆ†æï¼‰ã®ä¸€ç¨®ã§ã™ã€‚

            #### ğŸ’¡ ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜
            * **ã“ã‚Œä½•ï¼Ÿ**: å›ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä¼¼ã¦ã„ã‚‹äººã‚’é›†ã‚ã¦ã€è‡ªå‹•çš„ã«ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒãƒ¼ãƒ ï¼‰ã‚’ä½œã‚‹æ‰‹æ³•ã§ã™ã€‚
            * **ç›®çš„**: ã€Œã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé¡§å®¢åˆ†é¡ï¼‰ã€ã‚’è¡Œã†ãŸã‚ã§ã™ã€‚æ€§åˆ¥ã‚„å¹´ä»£ã ã‘ã§ãªãã€ã€Œæ„è­˜ã€ã‚„ã€Œè¡Œå‹•ã€ã§åˆ†é¡ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šåˆºã•ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œã‚Œã¾ã™ã€‚
            * **ä½¿ã„æ–¹ã®ã‚³ãƒ„**: 
                * è¦å› åˆ†æã§ã€Œé‡è¦ã ã€ã¨ã‚ã‹ã£ãŸé …ç›®ã‚’ä½¿ã£ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ä½œã‚‹ã¨ã€æ„å‘³ã®ã‚ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ãŒã§ãã‚„ã™ã„ã§ã™ã€‚
                * ã‚°ãƒ«ãƒ¼ãƒ—åã¯ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’è¦‹ã¦äººé–“ãŒè€ƒãˆã¾ã™ï¼ˆä¾‹ï¼šã€Œä¾¡æ ¼é‡è¦–æ´¾ã€ã€Œå“è³ªé‡è¦–æ´¾ã€ãªã©ï¼‰ã€‚
            """)

    # --- ã‚¿ãƒ–7: AIåˆ†æã‚¢ã‚·ã‚¹ãƒˆ ---
    with tab7:
        st.header("ğŸ¤– AIåˆ†æã‚¢ã‚·ã‚¹ãƒˆ")
        
        st.subheader("Step 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼")
        st.markdown("ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€Geminiã‚„ChatGPTã«**CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜**ã—ãŸçŠ¶æ…‹ã§é€ä¿¡ã—ã¦ãã ã•ã„ã€‚")

        ai_prompt_text = """# ã€çµ¶å¯¾éµå®ˆã®ãƒ«ãƒ¼ãƒ«ã€‘
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€Œé•·ããªã‚‹ã®ã§çœç•¥ã—ã¾ã™ã€ã€Œä»£è¡¨çš„ãªãƒˆãƒƒãƒ—3ã‚’æŒ™ã’ã¾ã™ã€ã¨ã„ã£ãŸè¦ç´„ã‚„çœç•¥ã‚’**çµ¶å¯¾ã«ã—ãªã„ã§ãã ã•ã„**ã€‚
- æ¡ä»¶ã«åˆè‡´ã™ã‚‹çµæœãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€**è¦‹ã¤ã‹ã£ãŸæ•°ã ã‘ã€ã™ã¹ã¦æ¼ã‚‰ã•ãšåˆ—æŒ™**ã—ã¦ãã ã•ã„ã€‚

# ä¾é ¼
æ·»ä»˜ã®ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ç«‹æ¡ˆã®ãŸã‚ã®è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®4ã¤ã®åˆ†ææ‰‹æ³•ã‚’ç”¨ã„ã¦ã€ãƒˆãƒƒãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå…¨ä½“å‚¾å‘ï¼‰ã ã‘ã§ãªãã€é¡•è‘—ãªç‰¹å¾´ã‚„èˆˆå‘³æ·±ã„ç›¸é–¢ã‚’ã€Œã™ã¹ã¦ç¶²ç¾…çš„ã«ã€æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# åˆ†ææ‰‹æ³•ã¨è¦³ç‚¹
1. **ã‚¯ãƒ­ã‚¹é›†è¨ˆ**
   - å¹´ä»£ã€æ€§åˆ¥ã€å±…ä½å¹´æ•°ãªã©ã®åŸºæœ¬å±æ€§ã¨ã€æº€è¶³åº¦ã‚„æ„è­˜è¨­å•ã¨ã®æ›ã‘åˆã‚ã›ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
   - å„æ„è­˜ãƒ»æº€è¶³åº¦ã®Top2ï¼ˆè‚¯å®šæ´¾ï¼‰ã®å‰²åˆã®å·®åˆ†ãŒ15%ä»¥ä¸Šè¦‹ã‚‰ã‚ŒãŸçµ„ã¿åˆã‚ã›ã‚’**ã™ã¹ã¦**æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
   - ã•ã‚‰ã«ã€Bottom2ï¼ˆå¦å®šæ´¾ï¼‰ã®å‰²åˆã®å·®åˆ†ãŒ15%ä»¥ä¸Šè¦‹ã‚‰ã‚Œã‚‹çµ„ã¿åˆã‚ã›ã‚‚**ã™ã¹ã¦**æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

2. **ãƒ‰ãƒ©ã‚¤ãƒãƒ¼åˆ†æï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰**
   - **âš ï¸é‡è¦: ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå†…ã«ã‚ã‚‹ã€ŒçŠ¶æ…‹ã‚„æ„è­˜ã‚’å•ã†è¨­å•ï¼ˆä¾‹ï¼šæ„›ç€ã€ä½ã¿ã‚„ã™ã•ã€å¹¸ã›ã€è‰¯ãã—ãŸã„ã€è©•åˆ¤ãŒè‰¯ã„ç­‰ï¼‰ã€ã‚’ã€ã™ã¹ã¦ï¼ˆæœ€ä½ã§ã‚‚5é …ç›®ä»¥ä¸Šï¼‰ã€‘å€‹åˆ¥ã®ç›®çš„å¤‰æ•°ã¨ã—ã¦è¨­å®šã—ã€ãã‚Œãã‚Œå›å¸°åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚**
   - èª¬æ˜å¤‰æ•°ï¼šåœ°åŸŸè¦ç´ ã‚„æº€è¶³åº¦ã«é–¢ã™ã‚‹è¨­å•ï¼ˆQ6ç³»ãªã©ï¼‰ã‚’ã™ã¹ã¦æŠ•å…¥ã—ã¦ãã ã•ã„ã€‚
   - ãã‚Œãã‚Œã®ç›®çš„å¤‰æ•°ã«å¯¾ã—ã€På€¤ãŒæœ‰æ„ï¼ˆ< 0.05ï¼‰ãªã‚‚ã®ã®ã†ã¡ã€ã‚ªãƒƒã‚ºæ¯”ãŒ1.0ã‚ˆã‚Šå¤§ãããƒ—ãƒ©ã‚¹ã®å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹è¦ç´ ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°åŒ–ã—ã¦ã™ã¹ã¦åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚
   - åŒæ™‚ã«ã€ã‚ªãƒƒã‚ºæ¯”ãŒ0.8ä»¥ä¸‹ã®ãƒã‚¤ãƒŠã‚¹ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰ã®å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹è¦ç´ ã‚‚åˆ†ã‘ã¦ã™ã¹ã¦åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚

3. **æ±ºå®šæœ¨åˆ†æ**
   - ç›®çš„å¤‰æ•°ï¼šä¸Šè¨˜ã§è¨­å®šã—ãŸæ„è­˜ã«é–¢ã™ã‚‹é‡è¦æŒ‡æ¨™ï¼ˆ5é …ç›®ä»¥ä¸Šï¼‰
   - ã©ã®ã‚ˆã†ãªæ¡ä»¶ãŒé‡ãªã‚‹ã¨ã€ãã®é‡è¦æŒ‡æ¨™ãŒé«˜ããªã‚‹ï¼ˆã¾ãŸã¯ä½ããªã‚‹ï¼‰ã‹ã®åˆ†å²ãƒ«ãƒ¼ãƒ«ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚

4. **ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ**
   - å›ç­”å‚¾å‘ãŒä¼¼ã¦ã„ã‚‹å›ç­”è€…ã‚’ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ï¼ˆ3ã€œ5ã‚°ãƒ«ãƒ¼ãƒ—ç¨‹åº¦ï¼‰ã—ã¦ãã ã•ã„ã€‚
   - å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´ï¼ˆä½•ã«æº€è¶³ã—ã€ä½•ã«ä¸æº€ã‹ï¼‰ã¨ã€å‘½åï¼ˆãƒšãƒ«ã‚½ãƒŠåï¼‰ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

# å‡ºåŠ›å½¢å¼
1. **åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆäººé–“ãŒèª­ã‚€ç”¨ï¼‰**
   - å„åˆ†æã”ã¨ã«è¦‹å‡ºã—ã‚’ç«‹ã¦ã€ç®‡æ¡æ›¸ãã§è©³ç´°ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

2. **ã‚¢ãƒ—ãƒªé€£æºç”¨è¨­å®šãƒ‡ãƒ¼ã‚¿ï¼ˆJSONå½¢å¼ï¼‰**
   - **æœ€å¾Œã«å¿…ãš**ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å„åˆ†æã”ã¨ã«è¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
   - **âš ï¸ã€é‡è¦ã€‘ã€Œãƒ‰ãƒ©ã‚¤ãƒãƒ¼åˆ†æã€ã«ã¤ã„ã¦ã¯ã€åˆ†æã—ãŸã™ã¹ã¦ã®ç›®çš„å¤‰æ•°ï¼ˆæœ€ä½ã§ã‚‚5é …ç›®ä»¥ä¸Šï¼‰ã‚’JSONã®ãƒªã‚¹ãƒˆã«å«ã‚ã¦ãã ã•ã„ã€‚çµ¶å¯¾ã«3ã¤ã§çœç•¥ã—ãªã„ã§ãã ã•ã„ã€‚**
   - **åˆ—åã¯ã€CSVã®ãƒ˜ãƒƒãƒ€ãƒ¼ã«ã‚ã‚‹æ­£ç¢ºãªåç§°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚**

```json
{
  "cross_tab": [
    {"index": "åˆ—åA", "columns": "åˆ—åB"},
    {"index": "åˆ—åC", "columns": "åˆ—åD"}
    // ... è¦‹ã¤ã‘ãŸæ•°ã ã‘ã™ã¹ã¦è¨˜è¿°ã™ã‚‹
  ],
  "driver_analysis": [
    {"target": "æ„è­˜è¨­å•1", "features": ["èª¬æ˜å¤‰æ•°1", "èª¬æ˜å¤‰æ•°2", "èª¬æ˜å¤‰æ•°3"]},
    {"target": "æ„è­˜è¨­å•2", "features": ["èª¬æ˜å¤‰æ•°1", "èª¬æ˜å¤‰æ•°4"]},
    {"target": "æ„è­˜è¨­å•3", "features": ["èª¬æ˜å¤‰æ•°2", "èª¬æ˜å¤‰æ•°5"]},
    {"target": "æ„è­˜è¨­å•4", "features": ["èª¬æ˜å¤‰æ•°1", "èª¬æ˜å¤‰æ•°6"]},
    {"target": "æ„è­˜è¨­å•5", "features": ["èª¬æ˜å¤‰æ•°3", "èª¬æ˜å¤‰æ•°7"]}
    // ... åˆ†æã—ãŸç›®çš„å¤‰æ•°ã®æ•°ã ã‘ã™ã¹ã¦ï¼ˆæœ€ä½5ã¤ä»¥ä¸Šï¼‰è¨˜è¿°ã™ã‚‹
  ],
  "decision_tree": [
    {"target": "æ„è­˜è¨­å•1", "features": ["èª¬æ˜å¤‰æ•°1", "èª¬æ˜å¤‰æ•°2", "èª¬æ˜å¤‰æ•°3"]},
    {"target": "æ„è­˜è¨­å•2", "features": ["èª¬æ˜å¤‰æ•°1", "èª¬æ˜å¤‰æ•°4", "èª¬æ˜å¤‰æ•°5"]}
    // ...
  ],
  "clustering": [
    {"features": ["å¤‰æ•°1", "å¤‰æ•°2", "å¤‰æ•°3"], "n_clusters": 4},
    {"features": ["å¤‰æ•°1", "å¤‰æ•°4", "å¤‰æ•°5"], "n_clusters": 3}
  ]
}
```"""
        st_copy_to_clipboard(ai_prompt_text, "ğŸ“‹ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼", "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸï¼")
        st.code(ai_prompt_text, language="markdown")
        
        st.divider()

        st.subheader("Step 2: AIã®å›ç­”ã‚’è²¼ã‚Šä»˜ã‘")
        st.markdown("AIãŒå‡ºåŠ›ã—ãŸ**åˆ†æã‚µãƒãƒªãƒ¼å…¨ä½“ï¼ˆæœ€å¾Œã®JSONã¾ã§å«ã‚€ï¼‰**ã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")

        ai_input = st.text_area("ã“ã“ã«Geminiã®å›ç­”ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„", height=300)

        if ai_input:
            json_match = re.search(r'```json\s*({.*?})\s*```', ai_input, re.DOTALL)
            
            config_str = None
            if json_match:
                config_str = json_match.group(1)
            else:
                fallback_match = re.search(r'\{.*"cross_tab".*\}', ai_input, re.DOTALL)
                if fallback_match:
                    config_str = fallback_match.group(0)

            if config_str:
                try:
                    config = json.loads(config_str)
                    st.success("âœ… AIã‹ã‚‰ã®è¨­å®šãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Šã¾ã—ãŸï¼ä»¥ä¸‹ã®ãŠã™ã™ã‚ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ã€‚")
                    
                    col_ai_1, col_ai_2 = st.columns(2)
                    
                    if "cross_tab" in config and isinstance(config['cross_tab'], list):
                        with col_ai_1:
                            st.write(f"### ğŸ“ˆ ã‚¯ãƒ­ã‚¹é›†è¨ˆã®ãŠã™ã™ã‚ï¼ˆ{len(config['cross_tab'])}ä»¶ï¼‰")
                            for i, setting in enumerate(config['cross_tab']):
                                with st.expander(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {i+1}: {setting.get('index')} Ã— {setting.get('columns')}", expanded=False):
                                    st.write(f"**è¡Œ**: {setting.get('index')}")
                                    st.write(f"**åˆ—**: {setting.get('columns')}")
                                    st.button(
                                        f"ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}ã‚’é©ç”¨",
                                        key=f"btn_cross_{i}",
                                        on_click=cb_update_cross,
                                        args=(setting.get('index'), setting.get('columns'))
                                    )

                    if "driver_analysis" in config and isinstance(config['driver_analysis'], list):
                        with col_ai_2:
                            st.write(f"### ğŸš€ ãƒ‰ãƒ©ã‚¤ãƒãƒ¼åˆ†æã®ãŠã™ã™ã‚ï¼ˆ{len(config['driver_analysis'])}ä»¶ï¼‰")
                            for i, setting in enumerate(config['driver_analysis']):
                                tgt = setting.get('target')
                                feats = setting.get('features', [])
                                with st.expander(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {i+1}: {tgt}", expanded=False):
                                    st.write(f"**ç›®çš„**: {tgt}")
                                    st.caption(f"**è¦å› **: {', '.join(feats)}")
                                    valid_feats_reg = [f for f in feats if f in df.columns]
                                    st.button(
                                        f"ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}ã‚’é©ç”¨",
                                        key=f"btn_reg_{i}",
                                        on_click=cb_update_reg,
                                        args=(tgt, valid_feats_reg)
                                    )

                    col_ai_3, col_ai_4 = st.columns(2)

                    if "decision_tree" in config and isinstance(config['decision_tree'], list):
                        with col_ai_3:
                            st.write(f"### ğŸŒ³ æ±ºå®šæœ¨ã®ãŠã™ã™ã‚ï¼ˆ{len(config['decision_tree'])}ä»¶ï¼‰")
                            for i, setting in enumerate(config['decision_tree']):
                                tgt = setting.get('target')
                                feats = setting.get('features', [])
                                with st.expander(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {i+1}: {tgt}", expanded=False):
                                    st.write(f"**ç›®çš„**: {tgt}")
                                    st.caption(f"**è¦å› **: {', '.join(feats)}")
                                    valid_feats = [f for f in feats if f in df.columns]
                                    st.button(
                                        f"ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}ã‚’é©ç”¨",
                                        key=f"btn_tree_{i}",
                                        on_click=cb_update_tree,
                                        args=(tgt, valid_feats)
                                    )

                    if "clustering" in config and isinstance(config['clustering'], list):
                        with col_ai_4:
                            st.write(f"### ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã®ãŠã™ã™ã‚ï¼ˆ{len(config['clustering'])}ä»¶ï¼‰")
                            for i, setting in enumerate(config['clustering']):
                                feats = setting.get('features', [])
                                with st.expander(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {i+1}: {len(feats)}å¤‰æ•°ã§åˆ†é¡", expanded=False):
                                    st.caption(f"**å¤‰æ•°**: {', '.join(feats)}")
                                    valid_feats_cluster = [f for f in feats if f in df.columns]
                                    st.button(
                                        f"ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}ã‚’é©ç”¨",
                                        key=f"btn_cluster_{i}",
                                        on_click=cb_update_cluster,
                                        args=(valid_feats_cluster,)
                                    )

                except json.JSONDecodeError as e:
                    st.error(f"JSONã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚AIã®å‡ºåŠ›ãŒæ­£ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\nè©³ç´°ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("è¨­å®šãƒ‡ãƒ¼ã‚¿(JSON)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æŒ‡ç¤ºé€šã‚Šã«GeminiãŒå‡ºåŠ›ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

else:
    st.info("ğŸ‘ˆ å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
